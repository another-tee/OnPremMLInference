# OnPremMLInference
In this repository, I attempt to run the ML inferencing API on-premises using microk8s and Kubernetes.
